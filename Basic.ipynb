{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ebfdc40f",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "663bc591",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vizdoom import *\n",
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "import gym\n",
    "from gym import Env\n",
    "from gym.spaces import Discrete, Box\n",
    "import cv2\n",
    "# Import callback class from sb3\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "import os\n",
    "from stable_baselines3.common.vec_env import VecFrameStack\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common import policies\n",
    "# import ppo for training\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3 import DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0cf5673",
   "metadata": {},
   "source": [
    "# Basic Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e9561e2",
   "metadata": {},
   "source": [
    "# Testing the basic game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0850964",
   "metadata": {},
   "outputs": [],
   "source": [
    "game = DoomGame()\n",
    "game.load_config('VizDoom/scenarios/basic.cfg')\n",
    "game.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b7877fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the set of actions we can take in the environment\n",
    "actions = np.identity(3, dtype=np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d538a1ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([50.])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state = game.get_state()\n",
    "state.game_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "292e1c05",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -9.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: 97.0\n",
      "reward: -1.0\n",
      "Result: 51.0\n",
      "reward: -4.0\n",
      "reward: -9.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -9.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -9.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -9.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -9.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: 99.0\n",
      "Result: -30.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -9.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -9.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -9.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -9.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -9.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -9.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -9.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -9.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -9.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -9.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -9.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -9.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -9.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -9.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "Result: -370.0\n",
      "reward: -4.0\n",
      "reward: 99.0\n",
      "Result: 95.0\n",
      "reward: -4.0\n",
      "reward: -9.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -9.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -9.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -9.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: 99.0\n",
      "Result: -29.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -9.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -9.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -9.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -9.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -9.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -9.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -9.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -9.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: 99.0\n",
      "Result: -93.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -9.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: 97.0\n",
      "Result: 40.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: 99.0\n",
      "Result: 87.0\n",
      "reward: -4.0\n",
      "reward: -9.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -9.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: 98.0\n",
      "Result: 52.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: -4.0\n",
      "reward: 99.0\n",
      "Result: 71.0\n"
     ]
    }
   ],
   "source": [
    "episodes = 10 \n",
    "for episode in range(episodes): \n",
    "    # Create a new episode or game \n",
    "    game.new_episode()\n",
    "    # Check the game isn't done \n",
    "    while not game.is_episode_finished(): \n",
    "        # Get the game state \n",
    "        state = game.get_state()\n",
    "        # Get the game image \n",
    "        img = state.screen_buffer\n",
    "        # Get the game variables - ammo\n",
    "        info = state.game_variables\n",
    "        # Take an action\n",
    "        reward = game.make_action(random.choice(actions),4)\n",
    "        # Print rewward \n",
    "        print('reward:', reward) \n",
    "        time.sleep(0.02)\n",
    "    print('Result:', game.get_total_reward())\n",
    "    time.sleep(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "580bf5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "game.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "496d53eb",
   "metadata": {},
   "source": [
    "# Set-up openAI framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d50b163e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4eec5bee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available on this device!\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")          # a CUDA device object\n",
    "    print('CUDA is available on this device!')\n",
    "else:\n",
    "    device = torch.device(\"cpu\")           # a CPU device object\n",
    "    print('CUDA is not available on this device :(')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "05597169",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DOOM OpenAI Gym SIMPLE Environment\n",
    "class VizDoomGym(Env): \n",
    "    def __init__(self, render=False): \n",
    "        super().__init__()\n",
    "        self.game = DoomGame()\n",
    "         \n",
    "        #load basic configuration for simple environment\n",
    "        self.game.load_config('VizDoom/scenarios/basic.cfg')\n",
    "        \n",
    "        #Set visibility of game\n",
    "        if render == True: \n",
    "            self.game.set_window_visible(True)\n",
    "        else:\n",
    "            self.game.set_window_visible(False)\n",
    "        \n",
    "        # Start the game \n",
    "        self.game.init()\n",
    "        \n",
    "        # Create observation space\n",
    "        self.observation_space = Box(low=0, high=255, shape=(100,160,1), dtype=np.uint8) \n",
    "        \n",
    "        # Create action space\n",
    "        self.action_space = Discrete(3)\n",
    "        \n",
    "    # Create Step function\n",
    "    def step(self, action):\n",
    "        actions = np.identity(3)\n",
    "        reward = self.game.make_action(actions[action], 4) \n",
    "        \n",
    "        # Get information from the game\n",
    "        if self.game.get_state(): \n",
    "            state = self.game.get_state().screen_buffer\n",
    "            state = self.grayscale(state)\n",
    "            ammo = self.game.get_state().game_variables[0]\n",
    "            info = ammo\n",
    "        else: \n",
    "            state = np.zeros(self.observation_space.shape)\n",
    "            info = 0 \n",
    "        \n",
    "        info = {\"info\":info}\n",
    "        done = self.game.is_episode_finished()\n",
    "        \n",
    "        return state, reward, done, info \n",
    "    \n",
    "    # Render\n",
    "    def render(): \n",
    "        pass\n",
    "    \n",
    "    # Reset game\n",
    "    def reset(self): \n",
    "        self.game.new_episode()\n",
    "        state = self.game.get_state().screen_buffer\n",
    "        return self.grayscale(state)\n",
    "    \n",
    "    # Grayscale the game frame and resize it \n",
    "    def grayscale(self, observation):\n",
    "        gray = cv2.cvtColor(np.moveaxis(observation, 0, -1), cv2.COLOR_BGR2GRAY)\n",
    "        resize = cv2.resize(gray, (160,100), interpolation=cv2.INTER_CUBIC)\n",
    "        state = np.reshape(resize, (100,160,1))\n",
    "        return state\n",
    "    \n",
    "    # Call to close down the game\n",
    "    def close(self): \n",
    "        self.game.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "14c3e2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = VizDoomGym(render=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ab450bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e643666",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbbf13cc",
   "metadata": {},
   "source": [
    "# Create Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "449ea0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainAndLoggingCallback(BaseCallback):\n",
    "\n",
    "    def __init__(self, check_freq, save_path, verbose=1):\n",
    "        super(TrainAndLoggingCallback, self).__init__(verbose)\n",
    "        self.check_freq = check_freq\n",
    "        self.save_path = save_path\n",
    "\n",
    "    def _init_callback(self):\n",
    "        if self.save_path is not None:\n",
    "            os.makedirs(self.save_path, exist_ok=True)\n",
    "\n",
    "    def _on_step(self):\n",
    "        if self.n_calls % self.check_freq == 0:\n",
    "            model_path = os.path.join(self.save_path, 'model_{}'.format(self.n_calls))\n",
    "            self.model.save(model_path)\n",
    "\n",
    "        return True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c29694",
   "metadata": {},
   "source": [
    "# Hyper parameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "17b6d417",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7f42e3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimise_ppo(trial):\n",
    "    \"\"\" Learning hyperparamters we want to optimise\"\"\"\n",
    "    return {\n",
    "        'n_steps': int(trial.suggest_loguniform('n_steps', 32, 2048)),\n",
    "        'learning_rate': trial.suggest_loguniform('learning_rate', 1e-5, 1),\n",
    "        'ent_coef': trial.suggest_loguniform('ent_coef', 1e-8, 1e-1),\n",
    "    }\n",
    "\n",
    "def optimise_agent(trial):\n",
    "    model_params = optimise_ppo(trial)\n",
    "    env = VizDoomGym(render=False)\n",
    "    model = PPO('CnnPolicy', env, verbose =1, **model_params)\n",
    "    model.learn(25000)\n",
    "    \n",
    "    \n",
    "    rewards = []\n",
    "    n_episodes, reward_sum = 0, 0.0\n",
    "\n",
    "    obs = env.reset()\n",
    "    while n_episodes < 4:\n",
    "        action, _ = model.predict(obs)\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        reward_sum += reward\n",
    "\n",
    "        if done:\n",
    "            rewards.append(reward_sum)\n",
    "            reward_sum = 0.0\n",
    "            n_episodes += 1\n",
    "            obs = env.reset()\n",
    "\n",
    "    last_reward = np.mean(rewards)\n",
    "\n",
    "    return -1 * last_reward\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "55f2e075",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-04-06 17:37:50,513]\u001b[0m A new study created in memory with name: no-name-6299b18d-eb55-49cb-8e8e-2b82195977b9\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "study = optuna.create_study()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d0c225e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ga401\\AppData\\Local\\Temp\\ipykernel_30840\\3716317451.py:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'n_steps': int(trial.suggest_loguniform('n_steps', 32, 2048)),\n",
      "C:\\Users\\Ga401\\AppData\\Local\\Temp\\ipykernel_30840\\3716317451.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 1e-5, 1),\n",
      "C:\\Users\\Ga401\\AppData\\Local\\Temp\\ipykernel_30840\\3716317451.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'ent_coef': trial.suggest_loguniform('ent_coef', 1e-8, 1e-1),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env in a VecTransposeImage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ga401\\anaconda3\\lib\\site-packages\\stable_baselines3\\ppo\\ppo.py:145: UserWarning: You have specified a mini-batch size of 64, but because the `RolloutBuffer` is of size `n_steps * n_envs = 63`, after every 0 untruncated mini-batches, there will be a truncated mini-batch of size 63\n",
      "We recommend using a `batch_size` that is a factor of `n_steps * n_envs`.\n",
      "Info: (n_steps=63 and n_envs=1)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------\n",
      "| time/              |    |\n",
      "|    fps             | 39 |\n",
      "|    iterations      | 1  |\n",
      "|    time_elapsed    | 1  |\n",
      "|    total_timesteps | 63 |\n",
      "---------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 21.6          |\n",
      "|    ep_rew_mean          | -18           |\n",
      "| time/                   |               |\n",
      "|    fps                  | 31            |\n",
      "|    iterations           | 2             |\n",
      "|    time_elapsed         | 4             |\n",
      "|    total_timesteps      | 126           |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.1291493e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.1          |\n",
      "|    explained_variance   | -0.00108      |\n",
      "|    learning_rate        | 2.09e-05      |\n",
      "|    loss                 | 1.97e+03      |\n",
      "|    n_updates            | 10            |\n",
      "|    policy_gradient_loss | -0.00055      |\n",
      "|    value_loss           | 4.09e+03      |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 30.2         |\n",
      "|    ep_rew_mean          | -59.2        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 29           |\n",
      "|    iterations           | 3            |\n",
      "|    time_elapsed         | 6            |\n",
      "|    total_timesteps      | 189          |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 7.340833e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.1         |\n",
      "|    explained_variance   | 0.00102      |\n",
      "|    learning_rate        | 2.09e-05     |\n",
      "|    loss                 | 1.19e+03     |\n",
      "|    n_updates            | 20           |\n",
      "|    policy_gradient_loss | -0.000407    |\n",
      "|    value_loss           | 2.39e+03     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 20.9          |\n",
      "|    ep_rew_mean          | -5.58         |\n",
      "| time/                   |               |\n",
      "|    fps                  | 28            |\n",
      "|    iterations           | 4             |\n",
      "|    time_elapsed         | 8             |\n",
      "|    total_timesteps      | 252           |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 9.6599266e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.1          |\n",
      "|    explained_variance   | 0.00155       |\n",
      "|    learning_rate        | 2.09e-05      |\n",
      "|    loss                 | 1.73e+03      |\n",
      "|    n_updates            | 30            |\n",
      "|    policy_gradient_loss | -6.07e-05     |\n",
      "|    value_loss           | 3.48e+03      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 22.1          |\n",
      "|    ep_rew_mean          | -10.9         |\n",
      "| time/                   |               |\n",
      "|    fps                  | 29            |\n",
      "|    iterations           | 5             |\n",
      "|    time_elapsed         | 10            |\n",
      "|    total_timesteps      | 315           |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 4.8303416e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.1          |\n",
      "|    explained_variance   | 0.00372       |\n",
      "|    learning_rate        | 2.09e-05      |\n",
      "|    loss                 | 1.71e+03      |\n",
      "|    n_updates            | 40            |\n",
      "|    policy_gradient_loss | -0.000496     |\n",
      "|    value_loss           | 3.42e+03      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 22.1          |\n",
      "|    ep_rew_mean          | -10.9         |\n",
      "| time/                   |               |\n",
      "|    fps                  | 29            |\n",
      "|    iterations           | 6             |\n",
      "|    time_elapsed         | 12            |\n",
      "|    total_timesteps      | 378           |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00029788038 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.1          |\n",
      "|    explained_variance   | 0.000798      |\n",
      "|    learning_rate        | 2.09e-05      |\n",
      "|    loss                 | 1e+03         |\n",
      "|    n_updates            | 50            |\n",
      "|    policy_gradient_loss | -0.00248      |\n",
      "|    value_loss           | 2.02e+03      |\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m[W 2023-04-06 17:38:09,150]\u001b[0m Trial 0 failed with parameters: {'n_steps': 63.52288032807318, 'learning_rate': 2.0913273644322846e-05, 'ent_coef': 5.6402618165441294e-08} because of the following error: KeyboardInterrupt().\u001b[0m\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Ga401\\anaconda3\\lib\\site-packages\\optuna\\study\\_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"C:\\Users\\Ga401\\AppData\\Local\\Temp\\ipykernel_30840\\3716317451.py\", line 13, in optimise_agent\n",
      "    model.learn(25000)\n",
      "  File \"C:\\Users\\Ga401\\anaconda3\\lib\\site-packages\\stable_baselines3\\ppo\\ppo.py\", line 307, in learn\n",
      "    return super().learn(\n",
      "  File \"C:\\Users\\Ga401\\anaconda3\\lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py\", line 248, in learn\n",
      "    continue_training = self.collect_rollouts(self.env, callback, self.rollout_buffer, n_rollout_steps=self.n_steps)\n",
      "  File \"C:\\Users\\Ga401\\anaconda3\\lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py\", line 166, in collect_rollouts\n",
      "    actions, values, log_probs = self.policy(obs_tensor)\n",
      "  File \"C:\\Users\\Ga401\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"C:\\Users\\Ga401\\anaconda3\\lib\\site-packages\\stable_baselines3\\common\\policies.py\", line 625, in forward\n",
      "    features = self.extract_features(obs)\n",
      "  File \"C:\\Users\\Ga401\\anaconda3\\lib\\site-packages\\stable_baselines3\\common\\policies.py\", line 648, in extract_features\n",
      "    return super().extract_features(obs, self.features_extractor)\n",
      "  File \"C:\\Users\\Ga401\\anaconda3\\lib\\site-packages\\stable_baselines3\\common\\policies.py\", line 141, in extract_features\n",
      "    preprocessed_obs = preprocess_obs(obs, self.observation_space, normalize_images=self.normalize_images)\n",
      "  File \"C:\\Users\\Ga401\\anaconda3\\lib\\site-packages\\stable_baselines3\\common\\preprocessing.py\", line 109, in preprocess_obs\n",
      "    if normalize_images and is_image_space(observation_space):\n",
      "  File \"C:\\Users\\Ga401\\anaconda3\\lib\\site-packages\\stable_baselines3\\common\\preprocessing.py\", line 55, in is_image_space\n",
      "    incorrect_bounds = np.any(observation_space.low != 0) or np.any(observation_space.high != 255)\n",
      "  File \"<__array_function__ internals>\", line 177, in any\n",
      "KeyboardInterrupt\n",
      "\u001b[33m[W 2023-04-06 17:38:09,157]\u001b[0m Trial 0 failed with value None.\u001b[0m\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[42], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mstudy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimise_agent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\optuna\\study\\study.py:425\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[1;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m    321\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize\u001b[39m(\n\u001b[0;32m    322\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    323\u001b[0m     func: ObjectiveFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    330\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    331\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    332\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[0;32m    333\u001b[0m \n\u001b[0;32m    334\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    422\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[0;32m    423\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 425\u001b[0m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    426\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    427\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    428\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    429\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    430\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    431\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    432\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    433\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    434\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    435\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\optuna\\study\\_optimize.py:66\u001b[0m, in \u001b[0;36m_optimize\u001b[1;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m---> 66\u001b[0m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     74\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     75\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     76\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     77\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     78\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     79\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\optuna\\study\\_optimize.py:163\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[1;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[0;32m    160\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    162\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 163\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    165\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[0;32m    166\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[0;32m    167\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[0;32m    168\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[0;32m    169\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\optuna\\study\\_optimize.py:251\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    244\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    247\u001b[0m     frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[0;32m    248\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[0;32m    250\u001b[0m ):\n\u001b[1;32m--> 251\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[0;32m    252\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\optuna\\study\\_optimize.py:200\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[0;32m    199\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 200\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    201\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    202\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[0;32m    203\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "Cell \u001b[1;32mIn[40], line 13\u001b[0m, in \u001b[0;36moptimise_agent\u001b[1;34m(trial)\u001b[0m\n\u001b[0;32m     11\u001b[0m env \u001b[38;5;241m=\u001b[39m VizDoomGym(render\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     12\u001b[0m model \u001b[38;5;241m=\u001b[39m PPO(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCnnPolicy\u001b[39m\u001b[38;5;124m'\u001b[39m, env, verbose \u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_params)\n\u001b[1;32m---> 13\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m25000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m rewards \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     17\u001b[0m n_episodes, reward_sum \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0.0\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\stable_baselines3\\ppo\\ppo.py:307\u001b[0m, in \u001b[0;36mPPO.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    297\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\n\u001b[0;32m    298\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfPPO,\n\u001b[0;32m    299\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    304\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    305\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfPPO:\n\u001b[1;32m--> 307\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    308\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    309\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    310\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    311\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    312\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    313\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    314\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:248\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    244\u001b[0m callback\u001b[38;5;241m.\u001b[39mon_training_start(\u001b[38;5;28mlocals\u001b[39m(), \u001b[38;5;28mglobals\u001b[39m())\n\u001b[0;32m    246\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m<\u001b[39m total_timesteps:\n\u001b[1;32m--> 248\u001b[0m     continue_training \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect_rollouts\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrollout_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_rollout_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    250\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m continue_training \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[0;32m    251\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:166\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.collect_rollouts\u001b[1;34m(self, env, callback, rollout_buffer, n_rollout_steps)\u001b[0m\n\u001b[0;32m    163\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m th\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m    164\u001b[0m     \u001b[38;5;66;03m# Convert to pytorch tensor or to TensorDict\u001b[39;00m\n\u001b[0;32m    165\u001b[0m     obs_tensor \u001b[38;5;241m=\u001b[39m obs_as_tensor(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_last_obs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m--> 166\u001b[0m     actions, values, log_probs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    167\u001b[0m actions \u001b[38;5;241m=\u001b[39m actions\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m    169\u001b[0m \u001b[38;5;66;03m# Rescale and perform action\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\stable_baselines3\\common\\policies.py:625\u001b[0m, in \u001b[0;36mActorCriticPolicy.forward\u001b[1;34m(self, obs, deterministic)\u001b[0m\n\u001b[0;32m    617\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    618\u001b[0m \u001b[38;5;124;03mForward pass in all the networks (actor and critic)\u001b[39;00m\n\u001b[0;32m    619\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    622\u001b[0m \u001b[38;5;124;03m:return: action, value and log probability of the action\u001b[39;00m\n\u001b[0;32m    623\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    624\u001b[0m \u001b[38;5;66;03m# Preprocess the observation if needed\u001b[39;00m\n\u001b[1;32m--> 625\u001b[0m features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    626\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshare_features_extractor:\n\u001b[0;32m    627\u001b[0m     latent_pi, latent_vf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp_extractor(features)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\stable_baselines3\\common\\policies.py:648\u001b[0m, in \u001b[0;36mActorCriticPolicy.extract_features\u001b[1;34m(self, obs)\u001b[0m\n\u001b[0;32m    641\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    642\u001b[0m \u001b[38;5;124;03mPreprocess the observation if needed and extract features.\u001b[39;00m\n\u001b[0;32m    643\u001b[0m \n\u001b[0;32m    644\u001b[0m \u001b[38;5;124;03m:param obs: Observation\u001b[39;00m\n\u001b[0;32m    645\u001b[0m \u001b[38;5;124;03m:return: the output of the features extractor(s)\u001b[39;00m\n\u001b[0;32m    646\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    647\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshare_features_extractor:\n\u001b[1;32m--> 648\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeatures_extractor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    649\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    650\u001b[0m     pi_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mextract_features(obs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpi_features_extractor)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\stable_baselines3\\common\\policies.py:141\u001b[0m, in \u001b[0;36mBaseModel.extract_features\u001b[1;34m(self, obs, features_extractor)\u001b[0m\n\u001b[0;32m    139\u001b[0m features_extractor \u001b[38;5;241m=\u001b[39m features_extractor \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeatures_extractor\n\u001b[0;32m    140\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m features_extractor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo features extractor was set\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 141\u001b[0m preprocessed_obs \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocess_obs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobservation_space\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormalize_images\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalize_images\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    142\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m features_extractor(preprocessed_obs)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\stable_baselines3\\common\\preprocessing.py:109\u001b[0m, in \u001b[0;36mpreprocess_obs\u001b[1;34m(obs, observation_space, normalize_images)\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;124;03mPreprocess observation to be to a neural network.\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;124;03mFor images, it normalizes the values by dividing them by 255 (to have values in [0, 1])\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    106\u001b[0m \u001b[38;5;124;03m:return:\u001b[39;00m\n\u001b[0;32m    107\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(observation_space, spaces\u001b[38;5;241m.\u001b[39mBox):\n\u001b[1;32m--> 109\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m normalize_images \u001b[38;5;129;01mand\u001b[39;00m \u001b[43mis_image_space\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobservation_space\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    110\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m obs\u001b[38;5;241m.\u001b[39mfloat() \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m255.0\u001b[39m\n\u001b[0;32m    111\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obs\u001b[38;5;241m.\u001b[39mfloat()\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\stable_baselines3\\common\\preprocessing.py:55\u001b[0m, in \u001b[0;36mis_image_space\u001b[1;34m(observation_space, check_channels, normalized_image)\u001b[0m\n\u001b[0;32m     52\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;66;03m# Check the value range\u001b[39;00m\n\u001b[1;32m---> 55\u001b[0m incorrect_bounds \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39many(observation_space\u001b[38;5;241m.\u001b[39mlow \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43many\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobservation_space\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhigh\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m!=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m255\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_bounds \u001b[38;5;129;01mand\u001b[39;00m incorrect_bounds:\n\u001b[0;32m     57\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32m<__array_function__ internals>:177\u001b[0m, in \u001b[0;36many\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "study.optimize(optimise_agent, n_trials = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e7c91027",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_steps': 1247.3982614438928, 'learning_rate': 0.00020658149856258678, 'ent_coef': 0.01390447360746765}\n"
     ]
    }
   ],
   "source": [
    "print(study.best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5abb2fe",
   "metadata": {},
   "source": [
    "# Training Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f593c42",
   "metadata": {},
   "source": [
    "-PPO sensitive to learning rate and batch size  \n",
    "-DQN sensitive to learning rate, Exploration rate, Batch size, Replay Buffer,Discount Factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "65e1af7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = VizDoomGym(render=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "935bfb22",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT_DIR = './model/basic_model/PPO_model_basic'\n",
    "LOG_DIR = './model_log/log_basic'\n",
    "\n",
    "callback = TrainAndLoggingCallback(check_freq=10000, save_path=CHECKPOINT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cec72f98",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env in a VecTransposeImage.\n"
     ]
    }
   ],
   "source": [
    "model = PPO('CnnPolicy', env, tensorboard_log=LOG_DIR, verbose=1,\n",
    "            learning_rate=0.00015, n_steps=2048, batch_size = 64, \n",
    "            ent_coef=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ac940d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to ./model_log/log_basic\\PPO_8\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 27.3     |\n",
      "|    ep_rew_mean     | -48.5    |\n",
      "| time/              |          |\n",
      "|    fps             | 32       |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 62       |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 22.6        |\n",
      "|    ep_rew_mean          | -29.3       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 31          |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 128         |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009979952 |\n",
      "|    clip_fraction        | 0.121       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.09       |\n",
      "|    explained_variance   | 0.000162    |\n",
      "|    learning_rate        | 0.00015     |\n",
      "|    loss                 | 858         |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.00206    |\n",
      "|    value_loss           | 2.63e+03    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 26.5        |\n",
      "|    ep_rew_mean          | -49.1       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 31          |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 193         |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013482701 |\n",
      "|    clip_fraction        | 0.274       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.07       |\n",
      "|    explained_variance   | 0.265       |\n",
      "|    learning_rate        | 0.00015     |\n",
      "|    loss                 | 1.12e+03    |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | 0.000757    |\n",
      "|    value_loss           | 2.86e+03    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 24.4        |\n",
      "|    ep_rew_mean          | -36         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 31          |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 257         |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.034615144 |\n",
      "|    clip_fraction        | 0.297       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.06       |\n",
      "|    explained_variance   | 0.507       |\n",
      "|    learning_rate        | 0.00015     |\n",
      "|    loss                 | 1.42e+03    |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | 0.00523     |\n",
      "|    value_loss           | 2.67e+03    |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 26.1       |\n",
      "|    ep_rew_mean          | -52        |\n",
      "| time/                   |            |\n",
      "|    fps                  | 29         |\n",
      "|    iterations           | 5          |\n",
      "|    time_elapsed         | 342        |\n",
      "|    total_timesteps      | 10240      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01737259 |\n",
      "|    clip_fraction        | 0.248      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.03      |\n",
      "|    explained_variance   | 0.45       |\n",
      "|    learning_rate        | 0.00015    |\n",
      "|    loss                 | 1.18e+03   |\n",
      "|    n_updates            | 40         |\n",
      "|    policy_gradient_loss | 0.000829   |\n",
      "|    value_loss           | 3.29e+03   |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 17.3        |\n",
      "|    ep_rew_mean          | -0.16       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 29          |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 413         |\n",
      "|    total_timesteps      | 12288       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016286943 |\n",
      "|    clip_fraction        | 0.274       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.987      |\n",
      "|    explained_variance   | 0.637       |\n",
      "|    learning_rate        | 0.00015     |\n",
      "|    loss                 | 1.19e+03    |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | 0.00358     |\n",
      "|    value_loss           | 2.98e+03    |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 16.7       |\n",
      "|    ep_rew_mean          | 3.65       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 30         |\n",
      "|    iterations           | 7          |\n",
      "|    time_elapsed         | 477        |\n",
      "|    total_timesteps      | 14336      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01659095 |\n",
      "|    clip_fraction        | 0.216      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.907     |\n",
      "|    explained_variance   | 0.744      |\n",
      "|    learning_rate        | 0.00015    |\n",
      "|    loss                 | 1.15e+03   |\n",
      "|    n_updates            | 60         |\n",
      "|    policy_gradient_loss | 0.0048     |\n",
      "|    value_loss           | 2.72e+03   |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 20.4        |\n",
      "|    ep_rew_mean          | -22.7       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 30          |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 541         |\n",
      "|    total_timesteps      | 16384       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.031555526 |\n",
      "|    clip_fraction        | 0.29        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.931      |\n",
      "|    explained_variance   | 0.67        |\n",
      "|    learning_rate        | 0.00015     |\n",
      "|    loss                 | 1.94e+03    |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | 0.015       |\n",
      "|    value_loss           | 3.66e+03    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 20.6        |\n",
      "|    ep_rew_mean          | -20.7       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 30          |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 603         |\n",
      "|    total_timesteps      | 18432       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026617918 |\n",
      "|    clip_fraction        | 0.326       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.884      |\n",
      "|    explained_variance   | 0.799       |\n",
      "|    learning_rate        | 0.00015     |\n",
      "|    loss                 | 483         |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | 0.0162      |\n",
      "|    value_loss           | 1.87e+03    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 12.8        |\n",
      "|    ep_rew_mean          | 30.8        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 30          |\n",
      "|    iterations           | 10          |\n",
      "|    time_elapsed         | 675         |\n",
      "|    total_timesteps      | 20480       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014498617 |\n",
      "|    clip_fraction        | 0.292       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.914      |\n",
      "|    explained_variance   | 0.663       |\n",
      "|    learning_rate        | 0.00015     |\n",
      "|    loss                 | 1.65e+03    |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | 0.0098      |\n",
      "|    value_loss           | 3.86e+03    |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 11.1        |\n",
      "|    ep_rew_mean          | 46.3        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 30          |\n",
      "|    iterations           | 11          |\n",
      "|    time_elapsed         | 740         |\n",
      "|    total_timesteps      | 22528       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.052591946 |\n",
      "|    clip_fraction        | 0.343       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.859      |\n",
      "|    explained_variance   | 0.64        |\n",
      "|    learning_rate        | 0.00015     |\n",
      "|    loss                 | 1.71e+03    |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | 0.00806     |\n",
      "|    value_loss           | 4.09e+03    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 12.2        |\n",
      "|    ep_rew_mean          | 35.6        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 30          |\n",
      "|    iterations           | 12          |\n",
      "|    time_elapsed         | 797         |\n",
      "|    total_timesteps      | 24576       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.045214247 |\n",
      "|    clip_fraction        | 0.333       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.75       |\n",
      "|    explained_variance   | 0.467       |\n",
      "|    learning_rate        | 0.00015     |\n",
      "|    loss                 | 1.09e+03    |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | 0.00826     |\n",
      "|    value_loss           | 2.32e+03    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 8.8         |\n",
      "|    ep_rew_mean          | 59.1        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 30          |\n",
      "|    iterations           | 13          |\n",
      "|    time_elapsed         | 865         |\n",
      "|    total_timesteps      | 26624       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018980937 |\n",
      "|    clip_fraction        | 0.185       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.617      |\n",
      "|    explained_variance   | 0.643       |\n",
      "|    learning_rate        | 0.00015     |\n",
      "|    loss                 | 1.23e+03    |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.00202    |\n",
      "|    value_loss           | 2.49e+03    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 9.59        |\n",
      "|    ep_rew_mean          | 52.6        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 30          |\n",
      "|    iterations           | 14          |\n",
      "|    time_elapsed         | 930         |\n",
      "|    total_timesteps      | 28672       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.056082554 |\n",
      "|    clip_fraction        | 0.269       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.578      |\n",
      "|    explained_variance   | 0.583       |\n",
      "|    learning_rate        | 0.00015     |\n",
      "|    loss                 | 685         |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | 0.0172      |\n",
      "|    value_loss           | 1.23e+03    |\n",
      "-----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model.learn(total_timesteps=100000, callback=callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c996c3ca",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "74cd6ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import eval policy to test agent\n",
    "from stable_baselines3.common.evaluation import evaluate_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "8701581a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Reload model from disc\n",
    "model = PPO.load('./model/PPO2_model_basic/model_80000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f9014001",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1564"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "9ca27d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create rendered environment\n",
    "env = VizDoomGym(render=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "726073aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ga401\\anaconda3\\lib\\site-packages\\stable_baselines3\\common\\evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Evaluate mean reward for 10 games\n",
    "mean_reward, _ = evaluate_policy(model, env, n_eval_episodes=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "0a4043e1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "84.57"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "f5f9a5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create rendered environment\n",
    "env = VizDoomGym(render=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "a3dffdae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Reward for episode 95.0 is 0\n",
      "Total Reward for episode 83.0 is 1\n",
      "Total Reward for episode 95.0 is 2\n",
      "Total Reward for episode 75.0 is 3\n",
      "Total Reward for episode 75.0 is 4\n",
      "Total Reward for episode 95.0 is 5\n",
      "Total Reward for episode 71.0 is 6\n",
      "Total Reward for episode 95.0 is 7\n",
      "Total Reward for episode 67.0 is 8\n",
      "Total Reward for episode 87.0 is 9\n"
     ]
    }
   ],
   "source": [
    "for episode in range(10): \n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    result_reward = 0\n",
    "    while not done: \n",
    "        action, _ = model.predict(obs)\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        # time.sleep(0.20)\n",
    "        result_reward += reward\n",
    "    print('Episode {}: Total Reward is {}'.format(episode, result_reward))\n",
    "    time.sleep(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
